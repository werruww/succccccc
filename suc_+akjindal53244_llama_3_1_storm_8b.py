# -*- coding: utf-8 -*-
"""suc_+akjindal53244_Llama-3.1-Storm-8B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x-_0mHg4kpZLWJKTwEhkJ_iUpVARNbUu
"""





"""https://huggingface.co/blog/akjindal53244/llama31-storm8b"""

import transformers
import torch

model_id = "akjindal53244/Llama-3.1-Storm-8B"
pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device_map="auto",
)

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is 2+2?"}
]

outputs = pipeline(messages, max_new_tokens=128, do_sample=True, temperature=0.01, top_k=100, top_p=0.95)
print(outputs[0]["generated_text"][-1])

pip install 'litgpt[all]'
litgpt download akjindal53244/Llama-3.1-Storm-8B --model_name meta-llama/Meta-Llama-3.1-8B

from litgpt import LLM

llm = LLM.load(model="akjindal53244/Llama-3.1-Storm-8B")
llm.generate("What do Llamas eat?")

!pip install 'litgpt[all]'

"""https://github.com/Lightning-AI/litgpt"""



!pip install huggingface-hub'>=0.34.0,<1.0'

!litgpt download akjindal53244/Llama-3.1-Storm-8B --model_name meta-llama/Meta-Llama-3.1-8B

from litgpt import LLM

llm = LLM.load(model="meta-llama/Meta-Llama-3.1-8B")
llm.generate("What do Llamas eat?")