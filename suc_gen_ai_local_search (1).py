# -*- coding: utf-8 -*-
"""suc_Gen-AI-Local-Search.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rYDrj-UDQu1xYaeCiGlfUF1hmvrikfQk
"""



!pip install --upgrade "transformers>=4.43.2" torch==2.3.1 accelerate vllm==0.5.3.post1

from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

model_id = "akjindal53244/Llama-3.1-Storm-8B"  # FP8 model: "akjindal53244/Llama-3.1-Storm-8B-FP8-Dynamic"
num_gpus = 1

tokenizer = AutoTokenizer.from_pretrained(model_id)
llm = LLM(model=model_id, tensor_parallel_size=num_gpus)
sampling_params = SamplingParams(max_tokens=128, temperature=0.01, top_k=100, top_p=0.95)

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is 2+2?"}
]
prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize = False)
print(llm.generate([prompt], sampling_params)[0].outputs[0].text.strip())  # Expected Output: 2 + 2 = 4

"""https://huggingface.co/blog/akjindal53244/llama31-storm8b"""

import transformers
import torch

model_id = "akjindal53244/Llama-3.1-Storm-8B"
pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device_map="auto",
)

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is 2+2?"}
]

outputs = pipeline(messages, max_new_tokens=128, do_sample=True, temperature=0.01, top_k=100, top_p=0.95)
print(outputs[0]["generated_text"][-1])









curl -fsSL https://ollama.com/install.sh | sh
nohup ollama serve &
ollama pull llama3:8b
ollama list

!curl -fsSL https://ollama.com/install.sh | sh

"""https://huggingface.co/akjindal53244/Llama-3.1-Storm-8B-FP8-Dynamic

https://huggingface.co/blog/akjindal53244/llama31-storm8b

https://medium.com/@hemanthgk/using-ollama-to-build-a-local-search-engine-for-your-files-d8a88f55cd51

https://github.com/hemanthgk10/Gen-AI-Local-Search
"""

!ollama run ajindal/llama3.1-storm:8b

!pip install ollama

import ollama

tools = [{
      'type': 'function',
      'function': {
        'name': 'get_current_weather',
        'description': 'Get the current weather for a city',
        'parameters': {
          'type': 'object',
          'properties': {
            'city': {
              'type': 'string',
              'description': 'The name of the city',
            },
          },
          'required': ['city'],
        },
      },
    },
    {
      'type': 'function',
      'function': {
        'name': 'get_places_to_vist',
        'description': 'Get places to visit in a city',
        'parameters': {
          'type': 'object',
          'properties': {
            'city': {
              'type': 'string',
              'description': 'The name of the city',
            },
          },
          'required': ['city'],
        },
      },
    },
  ]

response = ollama.chat(
    model='ajindal/llama3.1-storm:8b',
    messages=[
        {'role': 'system', 'content': 'Do not answer to nay vulgar questions.'},
        {'role': 'user', 'content': 'What is the weather in Toronto and San Francisco?'}
        ],
    tools=tools
)

print(response['message'])  # Expected Response: {'role': 'assistant', 'content': "<tool_call>{'tool_name': 'get_current_weather', 'tool_arguments': {'city': 'Toronto'}}</tool_call>"}









!git clone https://github.com/hemanthgk10/Gen-AI-Local-Search.git

# Commented out IPython magic to ensure Python compatibility.
# %cd gen_ai_local_search

!git clone https://github.com/hemanthgk10/gen_ai_local_search.git

# Commented out IPython magic to ensure Python compatibility.
# %cd gen_ai_local_search

!pip install -r requirements.txt

"""Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù `.env` Ù„ØªØ®Ø²ÙŠÙ† Ù…ÙØ§ØªÙŠØ­ API Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø±Ø± Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Colab Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ."""

# Create a .env file and add your API keys
# For example:
# OPENAI_API_KEY="your-openai-api-key"
# OLLAMA_BASE_URL="http://localhost:11434"

!streamlit run main.py

!pip install pyngrok

from pyngrok import ngrok
import threading
import time

ngrok_token = "XXXXX"
ngrok.set_auth_token(ngrok_token)

# Start ngrok in a separate thread to avoid blocking
def start_ngrok():
    public_url = ngrok.connect(3000).public_url
    print(f"ğŸš€ Ngrok Tunnel Open: {public_url}")

ngrok_thread = threading.Thread(target=start_ngrok)
ngrok_thread.start()

# Wait for ngrok to start (optional)
time.sleep(5)

# Execute your node.js script
!node hello-world.js

!which node
mv aa.js aa.mjs

from pyngrok import ngrok
import threading
import time

ngrok_token = ""
ngrok.set_auth_token(ngrok_token)

# Start ngrok in a separate thread to avoid blocking
def start_ngrok():
    public_url = ngrok.connect(8501).public_url
    print(f"ğŸš€ Ngrok Tunnel Open: {public_url}")

ngrok_thread = threading.Thread(target=start_ngrok)
ngrok_thread.start()

# Wait for ngrok to start (optional)
time.sleep(5)

# Execute your node.js script
!streamlit run main.py

run in termenal
ollama serve

!ollama list

ØºÙŠØ± Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
/content/gen_ai_local_search/app/generate.py

Tell me about the challenges at Lacework IAC Security.